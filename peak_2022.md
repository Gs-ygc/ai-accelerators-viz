# 2022 IEEE-HPEC Paper Data


This page has a list of some of the data columns/fields that are included in the 2022 IEEE-HPEC paper: 

A. Reuther, P. Michaleas, M. Jones, V. Gadepally, S. Samsi and J. Kepner, "AI and ML Accelerator Survey and Trends," *2022 IEEE High Performance Extreme Computing Conference (HPEC)*, 2022, pp. 1-10, \[[IEEE Xplore doi: coming in November] \[[ArXiv.org/abs/2210.04055](https://arxiv.org/abs/2210.04055)\]. 


For the full dataset in CSV format, please download \[[peak-accelerators-ieee-hpec-2022.csv](peak-accelerators-ieee-hpec-2022.csv)\]. 

---

| Company | Product | Label | Peak Perf. (GOPs/ GFLOPs) | Peak Power (W) | Precision | Form Factor | References | 
| ------- | ------- | ----- | :-----------------------: | :------------: | :-------: | :---------: | ---------- | 
| Achronix | VectorPath S7t-VG6 | Achronix | 86000 | 300 | int8 | Card | \[[www.eetimes.com](https://www.eetimes.com/fpga-acceleration-card-delivers-on-bandwidth-speed-and-flexibility/)\] | 
| Adapteva | Epiphany-V | Adaptiva | 2050 | 29.26 | fp32 | Chip | \[[arxiv.org](https://arxiv.org/abs/1610.01832)\]  \[[doi.org](https://doi.org/10.1109/ACSSC.2014.7094761)\] | 
| Aimotive | aiWare3 | Aimotive | 100000 | 25 | int8 | Chip | \[[aimotive.com](https://aimotive.com/news/content/1223)\] | 
| AIStorm	 | AIStorm | AIStorm | 2500 | 0.225 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/startup-accelerates-ai-at-the-sensor/)\] | 
| Alibaba | HanGuang 800 | Alibaba | 314000 | 157.126 | int8 | Card | \[[medium.com](https://medium.com/syncedreview/alibabas-new-ai-chip-can-process-nearly-80k-images-per-second-63412dec22a3)\] | 
| AlphaIC | RAP-C | AlphaIC | 60000 | 40 | int8 | System | \[[www.alphaics.ai](https://www.alphaics.ai/alphaics-introduces-worlds-most-powerful-ai-platform-alphaedgetm-for-l2-driverless-cars-and-autonomous-systems/)\] | 
| AlphaIC | RAP-E | AlphaIC | 30000 | 3 | int8 | Chip | \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/indo-us-startup-preps-agent-based-ai-processor/page/0/1)\] | 
| Amazon | Inferentia | AWS | 128000 | 200 | int8 | Card | \[[perspectives.mvdirona.com](https://perspectives.mvdirona.com/2018/11/aws-inferentia-machine-learning-processor/)\]  \[[www.cloudmanagementinsider.com](https://www.cloudmanagementinsider.com/amazon-inferentia-for-machine-learning-and-artificial-intelligence/)\] | 
| AMD | Radeon Instinct MI6 | AMD-MI8 | 8190 | 150 | fp16 | Card | \[[blog.exxactcorp.com](https://blog.exxactcorp.com/taking-deeper-look-amd-radeon-instinct-gpus-deep-learning/)\] | 
| AMD | Radeon Instinct MI60 | AMD-MI60 | 29500 | 300 | fp16 | Card | \[[www.anandtech.com](https://www.anandtech.com/show/13562/amd-announces-radeon-instinct-mi60-mi50-accelerators-powered-by-7nm-vega)\] | 
| ARM | Ethos N77 | Ethos | 4100 | 0.8 | int8 | Chip | \[[fuse.wikichip.org](https://fuse.wikichip.org/news/3282/arm-ethos-is-for-ubiquitous-ai-at-the-edge/)\] | 
| Axelera | Axelera Test Core | Axelera | 39100 | 2.79 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/axelera-demos-ai-test-chip-after-taping-out-in-four-months/)\] | 
| Baidu | Baidu Kunlun 818-300 | Baidu | 281000 | 160 | int8 | Chip | \[[doi.org](https://doi.org/10.1109/ISSCC42613.2021.9366056)\]  \[[www.eetimes.com](https://www.eetimes.com/baidu-accelerator-rises-in-ai/)\]  \[[www.zdnet.com](https://www.zdnet.com/article/baidu-creates-kunlun-silicon-for-ai/)\] | 
| Bitmain | BM1880 | Bitmain | 1000 | 2.5 | int8 | Chip | \[[www.linleygroup.com](https://www.linleygroup.com/newsletters/newsletter_detail.php?num=5975&year=2019&tag=3)\] | 
| Blaize | El Cano | Blaize | 1600 | 7 | int8 | Card | \[[www.blaize.com](https://www.blaize.com/wp-content/uploads/2020/09/Blaize-Ignites-Edge-AI-Performance.pdf)\] | 
| Cambricon | MLU100 | Cambricon | 64000 | 80 | fp16 | Card | \[[www.chinamoneynetwork.com](https://www.chinamoneynetwork.com/2018/05/04/chinese-ai-chip-maker-cambricon-unveils-new-cloud-based-smart-chip)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12815/cambricon-makers-of-huaweis-kirin-npu-ip-build-a-big-ai-chip-and-pcie-card)\] | 
| Cambricon | MLU100 | Cambricon | 128000 | 80 | int8 | Card | \[[www.chinamoneynetwork.com](https://www.chinamoneynetwork.com/2018/05/04/chinese-ai-chip-maker-cambricon-unveils-new-cloud-based-smart-chip)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12815/cambricon-makers-of-huaweis-kirin-npu-ip-build-a-big-ai-chip-and-pcie-card)\] | 
| Canaan | Kendrite K210 | Kendryte | 230 | 0.3 | int8 | Chip | \[[www.linleygroup.com](https://www.linleygroup.com/newsletters/newsletter_detail.php?num=5992)\] | 
| Cerebras | CS-1 | CS-1 | 1.64e+06 | 15000 | fp16 | System | \[[www.cerebras.net](https://www.cerebras.net/introducing-the-cerebras-cs-1-the-industrys-fastest-artificial-intelligence-computer/)\] | 
| Cerebras | CS-2 | CS-2 | 3.4e+06 | 23000 | fp16 | System | \[[www.hpcwire.com](https://www.hpcwire.com/2021/04/20/cerebras-doubles-ai-performance-with-second-gen-7nm-wafer-scale-engine/)\] | 
| Cornami | Cornami | Cornami | 419000 | 30 | fp16 | Chip | \[[cornami.com](https://cornami.com/1416-2/)\] | 
| Enflame | Cloudblazer T10 | Enflame | 8e-08 | 225 | bf16 | Card | \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/globalfoundries-aids-launch-chinese-ai-startup)\] | 
| Flex Logix | InferX X1 | FlexLogix | 7640 | 13.5 | int8 | Chip | \[[flex-logix.com](https://flex-logix.com/wp-content/uploads/2020/04/Accelerator-Evaluation-on-Real-Edge-Inference-Applications-04_03_2020.pdf)\] | 
| Google | TPU Edge | TPUedge | 4000 | 2 | int8 | System | \[[aiyprojects.withgoogle.com](https://aiyprojects.withgoogle.com/edge-tpu)\] | 
| Google | TPU1 | TPU1 | 92000 | 75 | int8 | Chip | \[[doi.org](https://doi.org/10.1145/3360307)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\] | 
| Google | TPU2 | TPU2 | 45000 | 280 | bf16.32 | Chip | \[[doi.org](https://doi.org/10.1145/3360307)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\] | 
| Google | TPU3 | TPU3 | 123000 | 450 | bf16.32 | Chip | \[[doi.org](https://doi.org/10.1145/3360307)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\] | 
| Google | TPU4i | TPU4i | 138000 | 175 | bf16.32 | Chip | \[[doi.org](https://doi.org/10.1109/ISCA52012.2021.00010)\] | 
| Google | TPU4 | TPU4 | 275000 | 192 | bf16.32 | Chip | \[[www.hpcwire.com](https://www.hpcwire.com/2022/05/16/google-clouds-new-tpu-v4-ml-hub-packs-9-exaflops-of-ai/)\] | 
| GraphCore | C2 | GraphCore | 125000 | 300 | fp16.32 | Card | \[[www.linleygroup.com](https://www.linleygroup.com/newsletters/newsletter_detail.php?num=5992)\]  \[[www.graphcore.ai](https://www.graphcore.ai/posts/preliminary-ipu-benchmarks-providing-previously-unseen-performance-for-a-range-of-machine-learning-applications)\] | 
| GraphCore | C2 | GraphCoreNode | 1.6e+06 | 2400 | fp16.32 | System | \[[www.graphcore.ai](https://www.graphcore.ai/hubfs/Lead%20gen%20assets/DSS8440%20IPU%20Server%20White%20Paper_2020.pdf)\] | 
| GraphCore | Colossus Mk2 | GraphCore2 | 250000 | 300 | bf16 | Card | \[[www.eetimes.com](https://www.eetimes.com/graphcore-takes-on-nvidia-with-second-gen-ai-accelerator/)\] | 
| GraphCore | Bow-2000 | GraphCoreBow | 350000 | 300 | bf16 | Card | \[[www.tomshardware.com](https://www.tomshardware.com/news/graphcore-tsmc-bow-ipu-3d-wafer-on-wafer-processor)\] | 
| GreenWaves | GAP8 | GAP8 | 22.7 | 0.1 | int8 | Chip | \[[greenwaves-technologies.com](https://greenwaves-technologies.com/gap8_gap9/)\]  \[[www.eejournal.com](https://www.eejournal.com/article/gap9-for-ml-at-the-edge/)\] | 
| GreenWaves | GAP9 | GAP9 | 151 | 0.64 | int8 | Chip | \[[greenwaves-technologies.com](https://greenwaves-technologies.com/gap8_gap9/)\]  \[[www.eejournal.com](https://www.eejournal.com/article/gap9-for-ml-at-the-edge/)\] | 
| Groq | Groq Node | GroqNode | 1.5e+06 | 3300 | fp16.32 | System | \[[www.nextplatform.com](https://www.nextplatform.com/2020/09/29/groq-shares-recipe-for-tsp-nodes-systems/)\] | 
| Groq | Groq Node | GroqNode | 6e+06 | 3300 | int8 | System | \[[www.nextplatform.com](https://www.nextplatform.com/2020/09/29/groq-shares-recipe-for-tsp-nodes-systems/)\] | 
| Groq | Tensor Streaming Processor | Groq | 205000 | 300 | fp16 | Card | \[[groq.com](http://groq.com/wp-content/uploads/2020/04/Groq-Rocks-NNs-Linley-Group-MPR-2020Jan06.pdf)\]  \[[doi.org](https://doi.org/10.1109/ISCA45697.2020.00023)\] | 
| Groq | Tensor Streaming Processor | Groq | 820000 | 300 | int8 | Card | \[[groq.com](http://groq.com/wp-content/uploads/2020/04/Groq-Rocks-NNs-Linley-Group-MPR-2020Jan06.pdf)\]  \[[doi.org](https://doi.org/10.1109/ISCA45697.2020.00023)\] | 
| Gyrfalcon | Gyrfalcon | Gyrfalcon | 2800 | 0.224 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/gyrfalcon-unveils-fourth-ai-accelerator-chip/)\] | 
| Gyrfalcon | Gyrfalcon | GyrfalconServer | 2.15e+06 | 900 | int8 | System | \[[www.hpcwire.com](https://www.hpcwire.com/off-the-wire/solidrun-gyrfalcon-develop-edge-optimized-ai-inference-server/)\] | 
| Habana | Gaudi | Gaudi | 50000 | 200 | fp16 | Card | \[[habana.ai](https://habana.ai/wp-content/uploads/2019/06/Habana-Offers-Gaudi-for-AI-Training.pdf)\]  \[[doi.org](https://doi.org/10.1109/MM.2020.2975185)\] | 
| Habana | Goya HL-1000 | Goya | 100000 | 150 | int8 | Card | \[[habana.ai](https://habana.ai/wp-content/uploads/2019/06/Habana-Offers-Gaudi-for-AI-Training.pdf)\]  \[[doi.org](https://doi.org/10.1109/MM.2020.2975185)\] | 
| Hailo | Hailo | Hailo-8 | 2690 | 9.3 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/details-of-hailo-ai-edge-accelerator-emerge/)\] | 
| Horizon Robotics | Journey2 | Journey2 | 4000 | 2 | int8 | Chip | \[[en.horizon.ai](https://en.horizon.ai/product/journey)\] | 
| Huawei HiSilicon | Ascend 310 | Ascend-310 | 8000 | 8 | fp16 | Chip | \[[e.huawei.com](https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-310)\] | 
| Huawei HiSilicon | Ascend 310 | Ascend-310 | 16000 | 8 | int8 | Chip | \[[e.huawei.com](https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-310)\] | 
| Huawei HiSilicon | Ascend 910 | Ascend-910 | 512000 | 310 | fp16 | Chip | \[[e.huawei.com](https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-910)\] | 
| Huawei HiSilicon | Ascend 910 | Ascend-910 | 512000 | 310 | int8 | Chip | \[[e.huawei.com](https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-910)\] | 
| IBM | TrueNorth | TrueNorth | 1890 | 0.5 | int8 | System | \[[www.top500.org](https://www.top500.org/news/ibm-finds-killer-app-for-truenorth-neuromorphic-chip/)\]  \[[doi.org](https://doi.org/10.1073/pnas.1604850113)\]  \[[doi.org](https://doi.org/10.1109/TCAD.2015.2474396)\] | 
| IBM | TrueNorth | TrueNorthSys | 1890 | 44 | int8 | System | \[[www.top500.org](https://www.top500.org/news/ibm-finds-killer-app-for-truenorth-neuromorphic-chip/)\]  \[[doi.org](https://doi.org/10.1073/pnas.1604850113)\]  \[[doi.org](https://doi.org/10.1109/TCAD.2015.2474396)\] | 
| IBM/NYU | NeuFlow | NeuFlow | 320 | 10 | int16 | Chip | \[[doi.org](https://doi.org/10.1109/CVPRW.2011.5981829)\] | 
| Institute for Computing Technology | DaDianNao | DaDianNao | 5590 | 15.97 | int16 | Chip | \[[cacm.acm.org](https://cacm.acm.org/magazines/2016/11/209123-diannao-family/)\]  \[[doi.org](https://doi.org/10.1109/MICRO.2014.58)\] | 
| Institute for Computing Technology | DianNao | DianNao | 452 | 0.485 | int16 | Chip | \[[cacm.acm.org](https://cacm.acm.org/magazines/2016/11/209123-diannao-family/)\] | 
| Institute for Computing Technology | PuDianNao | PuDianNao | 1060 | 0.596 | int16 | Chip | \[[cacm.acm.org](https://cacm.acm.org/magazines/2016/11/209123-diannao-family/)\]  \[[dl.acm.org](https://dl.acm.org/doi/10.1145/2775054.2694358)\] | 
| Institute for Computing Technology | ShiDianNao | ShiDianNao | 194 | 0.32 | int16 | Chip | \[[cacm.acm.org](https://cacm.acm.org/magazines/2016/11/209123-diannao-family/)\]  \[[doi.org](https://doi.org/10.1145/2749469.2750389)\] | 
| Intel | Arria 10 1150 | Arria | 283000 | 85 | fp16.32 | Chip | \[[arxiv.org](https://arxiv.org/abs/1807.06434)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/07/31/intel-fpga-architecture-focuses-on-deep-learning-inference/)\] | 
| Intel | Mobileye EyeQ5 | EyeQ5 | 12000 | 5 | int8 | Chip | \[[www.blaize.com](https://www.blaize.com/wp-content/uploads/2020/09/Blaize-Ignites-Edge-AI-Performance.pdf)\] | 
| Intel | Movidius Myriad X | MovidiusX | 1000 | 2 | int16 | Chip | \[[www.extremetech.com](https://www.extremetech.com/computing/254772-new-movidius-myriad-x-vpu-packs-custom-neural-compute-engine)\] | 
| Intel | Nervana Lake Crest | Nervana1 | 38000 | 210 | fp32 | Card | \[[newsroom.intel.com](https://newsroom.intel.com/editorials/artificial-intelligence-requires-holistic-approach/)\] | 
| Intel | Nervana Spring Crest | Nervana2 | 120000 | 210 | fp16.32 | Card | \[[newsroom.intel.com](https://newsroom.intel.com/editorials/artificial-intelligence-requires-holistic-approach/)\] | 
| Intel | Xeon Platinum 8180 | 2xXeon8180 | 4480 | 205 | fp32 | Chip | \[[www.anandtech.com](https://www.anandtech.com/show/14466/intel-xeon-cascade-lake-vs-nvidia-turing)\]  \[[www.cpu-world.com](http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208180.html)\] | 
| Intel | Xeon Platinum 8180 | 2xXeon8280 | 38700 | 205 | int8 | Chip | \[[www.anandtech.com](https://www.anandtech.com/show/14466/intel-xeon-cascade-lake-vs-nvidia-turing)\]  \[[www.cpu-world.com](http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208280.html)\] | 
| Kalray | Coolidge | Kalray | 3000 | 20 | fp16.32 | Chip | \[[www.european-processor-initiative.eu](https://www.european-processor-initiative.eu/dissemination-material/1259/)\]  \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/nxp-kalray-demo-coolidge-parallel-processor-bluebox)\] | 
| Kalray | Coolidge | Kalray | 1000 | 20 | fp32 | Chip | \[[www.european-processor-initiative.eu](https://www.european-processor-initiative.eu/dissemination-material/1259/)\]  \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/nxp-kalray-demo-coolidge-parallel-processor-bluebox)\] | 
| Kalray | Coolidge | Kalray | 24000 | 20 | int8.32 | Chip | \[[www.european-processor-initiative.eu](https://www.european-processor-initiative.eu/dissemination-material/1259/)\]  \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/nxp-kalray-demo-coolidge-parallel-processor-bluebox)\] | 
| Kneron | KL520 Neural Processing Unit | KL520 | 300 | 0.5 | int8 | Chip | \[[www.eetasia.com](https://www.eetasia.com/knerons-next-gen-edge-ai-chip-gets-40m-boost/)\] | 
| Kneron | KL720 | KL720 | 1400 | 1.556 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/kneron-attracts-strategic-investors/)\] | 
| Maxim | Max 78000 | Maxim | 56 | 0.028 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/maxim-debuts-homegrown-ai-accelerator-in-latest-ulp-soc/)\]  \[[www.linleygroup.com](_https://www.linleygroup.com/newsletters/newsletter_detail.php?num=6274&year=2021&tag=3)\]  \[[doi.org](_https://doi.org/10.1117/12.2622390)\] | 
| Microsoft | Brainwave | Brainwave | 2000 | 150 | int8 | Chip | \[[www.nextplatform.com](https://www.nextplatform.com/2017/08/24/drilling-microsofts-brainwave-soft-deep-leaning-chip/)\] | 
| MIT | Eyeriss | Eyeriss | 67.2 | 0.278 | int16 | Chip | \[[doi.org](https://doi.org/10.1109/MM.2017.265085944)\]  \[[doi.org](https://doi.org/10.1109/JSSC.2016.2616357)\]  \[[doi.org](https://doi.org/10.1109/JPROC.2017.2761740)\] | 
| MIT | Netcast | Netcast | 10000 | 0.001 | int8 | Chip | \[[arxiv.org](https://arxiv.org/ftp/arxiv/papers/2203/2203.05466.pdf)\] | 
| Mythic | M1076 | Mythic76 | 25000 | 3 | analog | Chip | \[[www.eetimes.com](https://www.eetimes.com/mythic-resizes-its-analog-ai-chip/)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/08/23/a-mythic-approach-to-deep-learning-inference/)\]  \[[medium.com](https://medium.com/mythic-ai/mythic-hot-chips-2018-637dfb9e38b7)\] | 
| Mythic | M1108 | Mythic108 | 35000 | 4 | analog | Chip | \[[www.eetimes.com](https://www.eetimes.com/mythic-resizes-its-analog-ai-chip/)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/08/23/a-mythic-approach-to-deep-learning-inference/)\]  \[[medium.com](https://medium.com/mythic-ai/mythic-hot-chips-2018-637dfb9e38b7)\] | 
| NovuMind | NovuTensor | NovuMind | 15000 | 15 | int8 | Chip | \[[moorinsightsstrategy.com](https://moorinsightsstrategy.com/wp-content/uploads/2019/05/NovuMind-An-Early-Entrant-in-AI-Silicon-By-Moor-Insights-And-Strategy.pdf)\]  \[[www.eetimes.com](https://www.eetimes.com/novuminds-ai-chip-sparks-controversy/)\] | 
| NVIDIA | Ampere A10 | A10 | 125000 | 150 | fp16.32 | Card | \[[www.nextplatform.com](https://www.nextplatform.com/2021/04/15/nvidia-rounds-out-ampere-lineup-with-two-new-accelerators/)\] | 
| NVIDIA | Ampere A100 | A100 | 312000 | 400 | fp16.32 | Card | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/)\] | 
| NVIDIA | Ampere A30 | A30 | 165000 | 165 | fp16.32 | Card | \[[www.nextplatform.com](https://www.nextplatform.com/2021/04/15/nvidia-rounds-out-ampere-lineup-with-two-new-accelerators/)\] | 
| NVIDIA | Ampere A40 | A40 | 150000 | 300 | fp16.32 | Card | \[[www.nextplatform.com](https://www.nextplatform.com/2021/04/15/nvidia-rounds-out-ampere-lineup-with-two-new-accelerators/)\] | 
| NVIDIA | DGX Station | DGX-Station | 480000 | 1500 | fp16.32 | System | \[[www.tomshardware.com](https://www.tomshardware.com/news/nvidia-volta-v100-dgx-1-hgx-1,34380.html)\] | 
| NVIDIA | DGX-1 | DGX-1 | 900000 | 3500 | fp16.32 | System | \[[www.tomshardware.com](https://www.tomshardware.com/news/nvidia-volta-v100-dgx-1-hgx-1,34380.html)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12587/nvidias-dgx2-sixteen-v100-gpus-30-tb-of-nvme-only-400k)\] | 
| NVIDIA | DGX-2 | DGX-2 | 1.92e+06 | 10000 | fp16.32 | System | \[[www.anandtech.com](https://www.anandtech.com/show/12587/nvidias-dgx2-sixteen-v100-gpus-30-tb-of-nvme-only-400k)\] | 
| NVIDIA | DGX-A100 | DGX-A100 | 5e+06 | 6500 | fp16.32 | System | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/defining-ai-innovation-with-dgx-a100/)\] | 
| NVIDIA | H100 | H100 | 1e+06 | 700 | fp16.32 | Card | \[[www.anandtech.com](https://www.anandtech.com/show/17327/nvidia-hopper-gpu-architecture-and-h100-accelerator-announced)\] | 
| NVIDIA | Jetson AGX Xavier | XavierAGX | 11000 | 30 | fp16 | System | \[[www.anandtech.com](https://www.anandtech.com/show/15070/nvidia-gives-jetson-xavier-a-trim-announces-nanosized-jetson-xavier-nx)\] | 
| NVIDIA | Jetson AGX Xavier | XavierAGX | 32000 | 30 | int8 | System | \[[www.anandtech.com](https://www.anandtech.com/show/15070/nvidia-gives-jetson-xavier-a-trim-announces-nanosized-jetson-xavier-nx)\] | 
| NVIDIA | Jetson NX Orin | OrinNX | 50000 | 25 | int8 | System | \[[www.hothardware.com](https://www.hothardware.com/news/nvidia-jetson-agx-orin)\]  \[[www.nvidia.com](_https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)\] | 
| NVIDIA | Jetson AGX Orin | OrinAGX | 138000 | 60 | int8 | System | \[[www.hothardware.com](https://www.hothardware.com/news/nvidia-jetson-agx-orin)\]  \[[www.nvidia.com](_https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)\] | 
| NVIDIA | Jetson TX1 | Jetson1 | 408 | 11.7 | fp16.32 | System | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/)\] | 
| NVIDIA | Jetson TX2 | Jetson2 | 580 | 12.8 | fp16.32 | System | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/)\] | 
| NVIDIA | Jetson Xavier NX | XavierNX | 6000 | 15 | fp16 | System | \[[www.anandtech.com](https://www.anandtech.com/show/15070/nvidia-gives-jetson-xavier-a-trim-announces-nanosized-jetson-xavier-nx)\] | 
| NVIDIA | Jetson Xavier NX | XavierNX | 21000 | 15 | int8 | System | \[[www.anandtech.com](https://www.anandtech.com/show/15070/nvidia-gives-jetson-xavier-a-trim-announces-nanosized-jetson-xavier-nx)\] | 
| NVIDIA | DRIVE AGX L2 | AGX-L2 | 200000 | 45 | int8 | System | \[[hothardware.com](https://hothardware.com/news/nvidia-drive-agx-pegasus-orin-ampere-next-gen-autonomous-cars)\] | 
| NVIDIA | DRIVE AGX L5 | AGX-L5 | 2e+06 | 800 | int8 | System | \[[hothardware.com](https://hothardware.com/news/nvidia-drive-agx-pegasus-orin-ampere-next-gen-autonomous-cars)\] | 
| NVIDIA | Pascal P100 | P100 | 21200 | 300 | fp16.32 | Card | \[[www.nvidia.com](https://www.nvidia.com/en-us/data-center/tesla-p100/)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12809/16gb-nvidia-tesla-v100-gets-reprieve-remains-in-production)\] | 
| NVIDIA | T4 | T4 | 131000 | 150 | int8 | Card | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/)\] | 
| NVIDIA | Volta V100 | V100 | 125000 | 300 | fp16.32 | Card | \[[www.nvidia.com](https://www.nvidia.com/en-us/data-center/tesla-v100/)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12809/16gb-nvidia-tesla-v100-gets-reprieve-remains-in-production)\] | 
| Perceive | Ergo | Perceive | 4000 | 0.0727 | int8 | Chip | \[[www.forbes.com](https://www.forbes.com/sites/tiriasresearch/2020/04/06/perceive-exits-stealth-with-super-efficient-machine-learning-chip-for-smarter-devices/#1b25ab646d9c)\] | 
| PEZY Computing | PEZY-SC2 | PEZY-SC2 | 8190 | 546.13 | fp32 | System | \[[fuse.wikichip.org](https://fuse.wikichip.org/news/191/the-2048-core-pezy-sc2-sets-a-green500-record/)\] | 
| Preferred Networks | MN-3 | Preferred-MN-3 | 5.24e-07 | 500 | fp16 | Card | \[[projects.preferred.jp](https://projects.preferred.jp/mn-core/en/)\]  \[[www.anandtech.com](https://www.anandtech.com/show/15177/preferred-networks-a-500-w-custom-pcie-card-using-3000-mm2-silicon)\] | 
| Quadric | q1-64 | Quadric | 6240 | 12 | int8 | Chip | \[[quadric.io](https://quadric.io/supercomputing.pdf)\] | 
| Qualcomm | Cloud AI 100 | Qcomm | 400000 | 75 | int8 | Card | \[[www.eetimes.com](https://www.eetimes.com/qualcomm-cloud-ai-100-promises-impressive-performance-per-watt-for-near-edge-ai/)\]  \[[www.eetimes.com](https://www.eetimes.com/qualcomm-targets-ai-inferencing-in-the-cloud/#)\] | 
| Rockchip | RK3399Pro | RK3399Pro | 2400 | 3 | int8 | Chip | \[[www.rock-chips.com](https://www.rock-chips.com/a/en/News/Press_Releases/2018/0108/869.html)\] | 
| SiMa.ai | SiMa.ai | SiMa.ai | 9120 | 4 | int8 | Chip | \[[www.linleygroup.com](https://www.linleygroup.com/uploads/sima-machine-learning-moves-to-the-edge-wp.pdf)\] | 
| Stanford | EIE | EIE | 102 | 0.6 | int16 | Chip | \[[ieeexplore.ieee.org](http://ieeexplore.ieee.org/document/7551397/)\] | 
| Stanford | TETRIS | Tetris | 128 | 8.42 | int16 | Chip | \[[dl.acm.org](https://dl.acm.org/doi/10.1145/3093337.3037702)\] | 
| Syntiant | NDP101 | Syntiant | 200 | 0.01 | int4.8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/tech-heavyweights-back-ai-chip-startup/)\]  \[[www.eetimes.com](https://www.eetimes.com/document.asp?doc_id=1334301)\] | 
| Tachyum | Prodigy | Tachyum | 1.2e+07 | 950 | fp16.32 | Chip | \[[www.tomshardware.com](https://www.tomshardware.com/news/tachyum-teases-128-core-cpu-57-ghz-950w-16-ddr5-channels)\] | 
| Tenstorrent | Tenstorrent | Tenstorrent | 368000 | 75 | int8 | Card | \[[www.tenstorrent.com](https://www.tenstorrent.com/wp-content/uploads/2020/04/Tenstorrent-Scales-AI-Performance.pdf)\] | 
| Tesla | Tesla Full Self-Driving Computer | Tesla | 72000 | 72 | int8 | System | \[[doi.org](https://doi.org/10.1109/MM.2020.2975764)\]  \[[en.wikichip.org](https://en.wikichip.org/wiki/tesla_(car_company)/fsd_chip)\] | 
| Texas Instruments | TDA4VM | TexInst | 8000 | 20 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/tis-first-automotive-soc-with-an-ai-accelerator-launches/)\]  \[[www.ti.com](https://www.ti.com/lit/gpn/tda4vm)\]  \[[www.linleygroup.com](https://www.linleygroup.com/newsletters/newsletter_detail.php?num=6130&year=2020&tag=3)\] | 
| Toshiba | 2015 | Toshiba | 20000 | 10 | int8 | System | \[[www.eetimes.com](https://www.eetimes.com/samsung-toshiba-detail-ai-chips/)\] | 
| Tsinghua | Tianjic | Tianjic | 1210 | 0.95 | int8 | Chip | \[[www.nature.com](http://www.nature.com/articles/s41586-019-1424-8)\] | 


---


Copyright 2022 MIT, Albert I. Reuther

| Untether | TsunAImi | TsunAImi | 2e+06 | 400 | int8 | Card | \[[www.linleygroup.com](https://www.linleygroup.com/newsletters/hewsletter_detail.php?num=6230)\] | 
| XMOS | xcore.ai | xcore.ai | 51.2 | 1 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/xmos-adapts-xcore-into-aiot-crossover-processor/#)\] | 
