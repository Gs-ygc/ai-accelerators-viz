# 2020 IEEE-HPEC Paper Data


This page has a list of some of the data columns/fields that are included in the 2020 IEEE-HPEC paper: 

A. Reuther, P. Michaleas, M. Jones, V. Gadepally, S. Samsi and J. Kepner, "Survey of Machine Learning Accelerators," *2020 IEEE High Performance Extreme Computing Conference (HPEC)*, 2020, pp. 1-12, \[[IEEE Xplore doi: 10.1109/HPEC43674.2020.9286149](https://doi.org/10.1109/HPEC43674.2020.9286149)\] \[[ArXiv.org/abs/2009.00993](https://arxiv.org/abs/2009.00993)\]. 


For the full dataset in CSV format, please download \[[peak-accelerators-ieee-hpec-2020.csv](peak_accelerators_ieee_hpec_2020.csv)\]. 

---

| Company | Product | Label | Peak Perf. (GOPs/ GFLOPs) | Peak Power (W) | Precision | Form Factor | References | 
| ------- | ------- | ----- | :-----------------------: | :------------: | :-------: | :---------: | ---------- | 
| Achronix | VectorPath S7t-VG6 | Achronix | 86000 | 300 | int8 | Card | \[[www.eetimes.com](https://www.eetimes.com/fpga-acceleration-card-delivers-on-bandwidth-speed-and-flexibility/)\] | 
| Adapteva | Epiphany-V | Adaptiva | 2050 | 29.26 | fp32 | Chip | \[[arxiv.org](https://arxiv.org/abs/1610.01832)\]  \[[doi.org](https://doi.org/10.1109/ACSSC.2014.7094761)\] | 
| Aimotive | aiWare3 | Aimotive | 100000 | 25 | int8 | Chip | \[[aimotive.com](https://aimotive.com/news/content/1223)\] | 
| AIStorm | AIStorm | AIStorm | 2500 | 0.225 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/startup-accelerates-ai-at-the-sensor/)\] | 
| Alibaba | Alibaba | Alibaba | 314000 | 157.126 | int8 | Card | \[[medium.com](https://medium.com/syncedreview/alibabas-new-ai-chip-can-process-nearly-80k-images-per-second-63412dec22a3)\] | 
| AlphaIC | RAP-C | AlphaIC | 60000 | 40 | int8 | System | \[[www.alphaics.ai](https://www.alphaics.ai/alphaics-introduces-worlds-most-powerful-ai-platform-alphaedgetm-for-l2-driverless-cars-and-autonomous-systems/)\] | 
| AlphaIC | RAP-E | AlphaIC | 30000 | 3 | int8 | Chip | \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/indo-us-startup-preps-agent-based-ai-processor/page/0/1)\] | 
| Amazon | Inferentia | Inferentia | 128000 | 200 | int8 | Card | \[[perspectives.mvdirona.com](https://perspectives.mvdirona.com/2018/11/aws-inferentia-machine-learning-processor/)\]  \[[www.cloudmanagementinsider.com](https://www.cloudmanagementinsider.com/amazon-inferentia-for-machine-learning-and-artificial-intelligence/)\] | 
| AMD | Radeon Instinct MI6 | AMD-MI8 | 8190 | 150 | fp16 | Card | \[[blog.exxactcorp.com](https://blog.exxactcorp.com/taking-deeper-look-amd-radeon-instinct-gpus-deep-learning/)\] | 
| AMD | Radeon Instinct MI60 | AMD-MI60 | 29500 | 300 | fp16 | Card | \[[www.anandtech.com](https://www.anandtech.com/show/13562/amd-announces-radeon-instinct-mi60-mi50-accelerators-powered-by-7nm-vega)\] | 
| ARM | Ethos N77 | Ethos | 4100 | 0.8 | int8 | Chip | \[[fuse.wikichip.org](https://fuse.wikichip.org/news/3282/arm-ethos-is-for-ubiquitous-ai-at-the-edge/)\] | 
| Baidu | Baidu Kunlun 818-300 | Baidu | 260000 | 100 | fp16.32 | Chip | \[[www.eetimes.com](https://www.eetimes.com/baidu-accelerator-rises-in-ai/)\]  \[[www.zdnet.com](https://www.zdnet.com/article/baidu-creates-kunlun-silicon-for-ai/)\] | 
| Bitmain | BM1880 | Bitmain | 1000 | 2.5 | int8 | Chip | \[[www.linleygroup.com](https://www.linleygroup.com/newsletters/newsletter_detail.php?num=5975&year=2019&tag=3)\] | 
| Cambricon | MLU100 | Cambricon | 128000 | 80 | int8 | Card | \[[www.chinamoneynetwork.com](https://www.chinamoneynetwork.com/2018/05/04/chinese-ai-chip-maker-cambricon-unveils-new-cloud-based-smart-chip)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12815/cambricon-makers-of-huaweis-kirin-npu-ip-build-a-big-ai-chip-and-pcie-card)\] | 
| Cambricon | MLU100 | Cambricon | 64000 | 80 | fp16 | Card | \[[www.chinamoneynetwork.com](https://www.chinamoneynetwork.com/2018/05/04/chinese-ai-chip-maker-cambricon-unveils-new-cloud-based-smart-chip)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12815/cambricon-makers-of-huaweis-kirin-npu-ip-build-a-big-ai-chip-and-pcie-card)\] | 
| Canaan | Kendrite K210 | Kendryte | 230 | 0.3 | int8 | Chip | \[[www.linleygroup.com](https://www.linleygroup.com/newsletters/newsletter_detail.php?num=5992)\] | 
| Cerebras | CS-1 | Cerebras | 1.64e+06 | 15000 | fp16 | Chip | \[[www.cerebras.net](https://www.cerebras.net/introducing-the-cerebras-cs-1-the-industrys-fastest-artificial-intelligence-computer/)\] | 
| Cornami | Cornami | Cornami | 419000 | 30 | fp16 | Chip | \[[cornami.com](https://cornami.com/1416-2/)\] | 
| Flex Logix | InferX X1 | FlexLogix | 7640 | 13.5 | int8 | Chip | \[[flex-logix.com](https://flex-logix.com/wp-content/uploads/2020/04/Accelerator-Evaluation-on-Real-Edge-Inference-Applications-04_03_2020.pdf)\] | 
| Google | TPU Edge | TPUedge | 4000 | 2 | int8 | System | \[[aiyprojects.withgoogle.com](https://aiyprojects.withgoogle.com/edge-tpu)\] | 
| Google | TPU1 | TPU1 | 23000 | 40 | int16 | Chip | \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\] | 
| Google | TPU2 | TPU2 | 45000 | 250 | fp16.32 | Chip | \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\] | 
| Google | TPU3 | TPU3 | 105000 | 200 | fp16.32 | Chip | \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/)\] | 
| GraphCore | C2 | GraphCoreC2 | 250000 | 300 | fp16.32 | Card | \[[www.linleygroup.com](https://www.linleygroup.com/newsletters/newsletter_detail.php?num=5992)\]  \[[www.graphcore.ai](https://www.graphcore.ai/posts/preliminary-ipu-benchmarks-providing-previously-unseen-performance-for-a-range-of-machine-learning-applications)\] | 
| GraphCore | C2 | GraphCoreNode | 1.6e+06 | 2400 | fp16.32 | System | \[[www.graphcore.ai](https://www.graphcore.ai/hubfs/Lead%20gen%20assets/DSS8440%20IPU%20Server%20White%20Paper_2020.pdf)\] | 
| GreenWaves | GAP9 | GreenWaves | 106 | 49.764 | int8 | Chip | \[[greenwaves-technologies.com](https://greenwaves-technologies.com/gap8_gap9/)\]  \[[www.eejournal.com](https://www.eejournal.com/article/gap9-for-ml-at-the-edge/)\] | 
| Groq | Tensor Streaming Processor | Groq | 820000 | 300 | int8 | Card | \[[groq.com](http://groq.com/wp-content/uploads/2020/04/Groq-Rocks-NNs-Linley-Group-MPR-2020Jan06.pdf)\]  \[[doi.org](https://doi.org/10.1109/ISCA45697.2020.00023)\] | 
| Groq | Tensor Streaming Processor | Groq | 205000 | 300 | fp16 | Card | \[[groq.com](http://groq.com/wp-content/uploads/2020/04/Groq-Rocks-NNs-Linley-Group-MPR-2020Jan06.pdf)\]  \[[doi.org](https://doi.org/10.1109/ISCA45697.2020.00023)\] | 
| Gyrfalcon | Gyrfalcon | Gyrfalcon | 2800 | 0.224 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/gyrfalcon-unveils-fourth-ai-accelerator-chip/)\] | 
| Gyrfalcon | Gyrfalcon | GyrfalconServer | 2.15e+06 | 900 | int8 | System | \[[www.hpcwire.com](https://www.hpcwire.com/off-the-wire/solidrun-gyrfalcon-develop-edge-optimized-ai-inference-server/)\] | 
| Habana | Gaudi | Gaudi | 50000 | 150 | fp16 | Card | \[[habana.ai](https://habana.ai/wp-content/uploads/2019/06/Habana-Offers-Gaudi-for-AI-Training.pdf)\]  \[[doi.org](https://doi.org/10.1109/MM.2020.2975185)\] | 
| Habana | Goya HL-1000 | Goya | 100000 | 150 | int8 | Card | \[[habana.ai](https://habana.ai/wp-content/uploads/2019/06/Habana-Offers-Gaudi-for-AI-Training.pdf)\]  \[[doi.org](https://doi.org/10.1109/MM.2020.2975185)\] | 
| Hailo | Hailo | Hailo-8 | 2690 | 9.3 | int8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/details-of-hailo-ai-edge-accelerator-emerge/)\] | 
| Horizon Robotics | Journey2 | Journey2 | 4000 | 2 | int8 | Chip | \[[en.horizon.ai](https://en.horizon.ai/product/journey)\] | 
| Huawei HiSilicon | Ascend 310 | Ascend-310 | 16000 | 8 | int8 | Chip | \[[e.huawei.com](https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-310)\] | 
| Huawei HiSilicon | Ascend 310 | Ascend-310 | 8000 | 8 | fp16 | Chip | \[[e.huawei.com](https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-310)\] | 
| Huawei HiSilicon | Ascend 910 | Ascend-910 | 512000 | 310 | int8 | Chip | \[[e.huawei.com](https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-910)\] | 
| Huawei HiSilicon | Ascend 910 | Ascend-910 | 512000 | 310 | fp16 | Chip | \[[e.huawei.com](https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-910)\] | 
| IBM | TrueNorth | TrueNorth | 1890 | 0.5 | int8 | System | \[[www.top500.org](https://www.top500.org/news/ibm-finds-killer-app-for-truenorth-neuromorphic-chip/)\]  \[[doi.org](https://doi.org/10.1073/pnas.1604850113)\]  \[[doi.org](https://doi.org/10.1109/TCAD.2015.2474396)\] | 
| IBM | TrueNorth | TrueNorthSys | 1890 | 44 | int8 | System | \[[www.top500.org](https://www.top500.org/news/ibm-finds-killer-app-for-truenorth-neuromorphic-chip/)\]  \[[doi.org](https://doi.org/10.1073/pnas.1604850113)\]  \[[doi.org](https://doi.org/10.1109/TCAD.2015.2474396)\] | 
| Institute for Computing Technology | DaDianNao | DaDianNao | 5590 | 15.97 | int16 | Chip | \[[cacm.acm.org](https://cacm.acm.org/magazines/2016/11/209123-diannao-family/)\]  \[[doi.org](https://doi.org/10.1109/MICRO.2014.58)\] | 
| Institute for Computing Technology | DianNao | DianNao | 452 | 0.485 | int16 | Chip | \[[cacm.acm.org](https://cacm.acm.org/magazines/2016/11/209123-diannao-family/)\] | 
| Institute for Computing Technology | PuDianNao | PuDianNao | 1060 | 0.596 | int16 | Chip | \[[cacm.acm.org](https://cacm.acm.org/magazines/2016/11/209123-diannao-family/)\]  \[[dl.acm.org](https://dl.acm.org/doi/10.1145/2775054.2694358)\] | 
| Institute for Computing Technology | ShiDianNao | ShiDianNao | 194 | 0.32 | int16 | Chip | \[[cacm.acm.org](https://cacm.acm.org/magazines/2016/11/209123-diannao-family/)\]  \[[doi.org](https://doi.org/10.1145/2749469.2750389)\] | 
| Intel | Arria 10 1150 | Arria | 283000 | 85 | fp16.32 | Chip | \[[arxiv.org](https://arxiv.org/abs/1807.06434)\]  \[[www.nextplatform.com](https://www.nextplatform.com/2018/07/31/intel-fpga-architecture-focuses-on-deep-learning-inference/)\] | 
| Intel | Movidius Myriad X | MovidiusX | 1000 | 2 | int16 | Chip | \[[www.extremetech.com](https://www.extremetech.com/computing/254772-new-movidius-myriad-x-vpu-packs-custom-neural-compute-engine)\] | 
| Intel | Nervana Lake Crest | Nervana1 | 38000 | 210 | fp32 | Card | \[[newsroom.intel.com](https://newsroom.intel.com/editorials/artificial-intelligence-requires-holistic-approach/)\] | 
| Intel | Nervana NNP L-1000 (Spring Crest) | Nervana2 | 120000 | 210 | fp16.32 | Card | \[[newsroom.intel.com](https://newsroom.intel.com/editorials/artificial-intelligence-requires-holistic-approach/)\] | 
| Intel | Xeon Platinum 8180 | 2xXeon8180 | 4480 | 410 | fp32 | Chip | \[[www.anandtech.com](https://www.anandtech.com/show/14466/intel-xeon-cascade-lake-vs-nvidia-turing)\]  \[[www.cpu-world.com](http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208180.html)\] | 
| Intel | Xeon Platinum 8180 | 2xXeon8280 | 38700 | 410 | int8 | Chip | \[[www.anandtech.com](https://www.anandtech.com/show/14466/intel-xeon-cascade-lake-vs-nvidia-turing)\]  \[[www.cpu-world.com](http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208280.html)\] | 
| Kalray | Coolidge | Kalray | 1000 | 20 | fp32 | Chip | \[[www.european-processor-initiative.eu](https://www.european-processor-initiative.eu/dissemination-material/1259/)\]  \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/nxp-kalray-demo-coolidge-parallel-processor-bluebox)\] | 
| Kalray | Coolidge | Kalray | 3000 | 20 | fp16.32 | Chip | \[[www.european-processor-initiative.eu](https://www.european-processor-initiative.eu/dissemination-material/1259/)\]  \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/nxp-kalray-demo-coolidge-parallel-processor-bluebox)\] | 
| Kalray | Coolidge | Kalray | 24000 | 20 | int8.32 | Chip | \[[www.european-processor-initiative.eu](https://www.european-processor-initiative.eu/dissemination-material/1259/)\]  \[[www.eenewsanalog.com](https://www.eenewsanalog.com/news/nxp-kalray-demo-coolidge-parallel-processor-bluebox)\] | 
| Kneron | KL520 Neural Processing Unit | Kneron | 300 | 0.5 | int8 | Chip | \[[www.eetasia.com](https://www.eetasia.com/knerons-next-gen-edge-ai-chip-gets-40m-boost/)\] | 
| Microsoft | Brainwave | Brainwave | 2000 | 150 | int8 | Chip | \[[www.nextplatform.com](https://www.nextplatform.com/2017/08/24/drilling-microsofts-brainwave-soft-deep-leaning-chip/)\] | 
| MIT | Eyeriss | Eyeriss | 67.2 | 0.278 | int16 | Chip | \[[doi.org](https://doi.org/10.1109/MM.2017.265085944)\]  \[[doi.org](https://doi.org/10.1109/JSSC.2016.2616357)\]  \[[doi.org](https://doi.org/10.1109/JPROC.2017.2761740)\] | 
| Mythic | Mythic | Mythic | 3600 | 2 | analog | Chip | \[[www.nextplatform.com](https://www.nextplatform.com/2018/08/23/a-mythic-approach-to-deep-learning-inference/)\]  \[[medium.com](https://medium.com/mythic-ai/mythic-hot-chips-2018-637dfb9e38b7)\] | 
| NovuMind | NovuTensor | NovuMind | 15000 | 15 | int8 | Chip | \[[moorinsightsstrategy.com](https://moorinsightsstrategy.com/wp-content/uploads/2019/05/NovuMind-An-Early-Entrant-in-AI-Silicon-By-Moor-Insights-And-Strategy.pdf)\]  \[[www.eetimes.com](https://www.eetimes.com/novuminds-ai-chip-sparks-controversy/)\] | 
| NVIDIA | Ampere A100 | A100 | 312000 | 400 | fp16.32 | Card | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/)\] | 
| NVIDIA | DGX Station | DGX-Station | 480000 | 1500 | fp16.32 | System | \[[www.tomshardware.com](https://www.tomshardware.com/news/nvidia-volta-v100-dgx-1-hgx-1,34380.html)\] | 
| NVIDIA | DGX-1 | DGX-1 | 900000 | 3500 | fp16.32 | System | \[[www.tomshardware.com](https://www.tomshardware.com/news/nvidia-volta-v100-dgx-1-hgx-1,34380.html)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12587/nvidias-dgx2-sixteen-v100-gpus-30-tb-of-nvme-only-400k)\] | 
| NVIDIA | DGX-2 | DGX-2 | 1.92e+06 | 10000 | fp16.32 | System | \[[www.anandtech.com](https://www.anandtech.com/show/12587/nvidias-dgx2-sixteen-v100-gpus-30-tb-of-nvme-only-400k)\] | 
| NVIDIA | DGX-A100 | DGX-A100 | 5e+06 | 6500 | fp16.32 | System | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/defining-ai-innovation-with-dgx-a100/)\] | 
| NVIDIA | Jetson Xavier | Xavier | 10000 | 30 | fp16 | System | \[[www.extremetech.com](https://www.extremetech.com/computing/270681-nvidias-jetson-xavier-stuffs-volta-performance-into-tiny-form-factor)\] | 
| NVIDIA | Jetson1 | Jetson1 | 408 | 11.7 | fp16.32 | System | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/)\] | 
| NVIDIA | Jetson2 | Jetson2 | 580 | 12.8 | fp16.32 | System | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/)\] | 
| NVIDIA | T4 | T4 | 131000 | 150 | int8 | Card | \[[devblogs.nvidia.com](https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/)\] | 
| NVIDIA | Volta V100 | V100 | 125000 | 300 | fp16.32 | Card | \[[www.nvidia.com](https://www.nvidia.com/en-us/data-center/tesla-v100/)\]  \[[www.anandtech.com](https://www.anandtech.com/show/12809/16gb-nvidia-tesla-v100-gets-reprieve-remains-in-production)\] | 
| Perceive | Ergo | Perceive | 4000 | 0.0727 | int8 | Chip | \[[www.forbes.com](https://www.forbes.com/sites/tiriasresearch/2020/04/06/perceive-exits-stealth-with-super-efficient-machine-learning-chip-for-smarter-devices/#1b25ab646d9c)\] | 
| PEZY Computing | PEZY-SC2 | PEZY-SC2 | 8190 | 546.13 | fp32 | System | \[[fuse.wikichip.org](https://fuse.wikichip.org/news/191/the-2048-core-pezy-sc2-sets-a-green500-record/)\] | 
| Preferred Networks | MN-3 | Preferred-MN-3 | 5.24e-07 | 500 | fp16 | Card | \[[projects.preferred.jp](https://projects.preferred.jp/mn-core/en/)\]  \[[www.anandtech.com](https://www.anandtech.com/show/15177/preferred-networks-a-500-w-custom-pcie-card-using-3000-mm2-silicon)\] | 
| Quadric | q1-64 | Quadric | 6240 | 12 | int8 | Chip | \[[quadric.io](https://quadric.io/supercomputing.pdf)\] | 
| Rockchip | RK3399Pro | RK3399Pro | 2400 | 3 | int8 | Chip | \[[www.rock-chips.com](https://www.rock-chips.com/a/en/News/Press_Releases/2018/0108/869.html)\] | 
| SiMa.ai | SiMa.ai | SiMa.ai | 9120 | 4 | int8 | Chip | \[[www.linleygroup.com](https://www.linleygroup.com/uploads/sima-machine-learning-moves-to-the-edge-wp.pdf)\] | 
| Syntiant | NDP101 | Syntiant | 200 | 0.01 | int4.8 | Chip | \[[www.eetimes.com](https://www.eetimes.com/tech-heavyweights-back-ai-chip-startup/)\]  \[[www.eetimes.com](https://www.eetimes.com/document.asp?doc_id=1334301)\] | 
| Tenstorrent | Tenstorrent | Tenstorrent | 368000 | 75 | int8 | Card | \[[www.tenstorrent.com](https://www.tenstorrent.com/wp-content/uploads/2020/04/Tenstorrent-Scales-AI-Performance.pdf)\] | 
| Tesla | Tesla Full Self-Driving Computer | Tesla | 72000 | 72 | int8 | System | \[[doi.org](https://doi.org/10.1109/MM.2020.2975764)\]  \[[en.wikichip.org](https://en.wikichip.org/wiki/tesla_(car_company)/fsd_chip)\] | 
| Toshiba | 2015 | Toshiba2015 | 20000 | 10 | int8 | System | \[[www.eetimes.com](https://www.eetimes.com/samsung-toshiba-detail-ai-chips/)\] | 
| Tsinghua | Tianjic | Tianjic | 1210 | 0.95 | int8 | Chip | \[[www.nature.com](http://www.nature.com/articles/s41586-019-1424-8)\] | 
| XMOS | xcore.ai | xcore.ai | 1.6 | 1 | fp16 | Chip | \[[www.eetimes.com](https://www.eetimes.com/xmos-adapts-xcore-into-aiot-crossover-processor/#)\] | 
| IBM/NYU | NeuFlow | NeuFlow | 320 | 10 | int16 | Chip | \[[doi.org](https://doi.org/10.1109/CVPRW.2011.5981829)\] | 
| Stanford | EIE | EIE | 102 | 0.6 | int16 | Chip | \[[ieeexplore.ieee.org](http://ieeexplore.ieee.org/document/7551397/)\] | 
| Stanford | TETRIS | Tetris | 128 | 8.42 | int16 | Chip | \[[dl.acm.org](https://dl.acm.org/doi/10.1145/3093337.3037702)\] | 

---


Copyright 2021 MIT, Albert I. Reuther
